{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment test funcs\n",
    "this nb is only made to test the deployment functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing libraries, modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath('..')\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "# own modules \n",
    "from utils.utils_load_data        import Loader\n",
    "from utils.utils_deployment_funcs import DeploymentFuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instances\n",
    "loader = Loader()\n",
    "deploy = DeploymentFuncs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0          1        17.99         10.38          122.80     1001.0   \n",
       "1          1        20.57         17.77          132.90     1326.0   \n",
       "2          1        19.69         21.25          130.00     1203.0   \n",
       "3          1        11.42         20.38           77.58      386.1   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33           184.60   \n",
       "1         0.1812  ...         24.99          23.41           158.80   \n",
       "2         0.2069  ...         23.57          25.53           152.50   \n",
       "3         0.2597  ...         14.91          26.50            98.87   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "\n",
       "[4 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original data\n",
    "df_raw = loader.load_data(file_name='breast_disease', dir= 'raw', copy= True)\n",
    "df_raw.drop(columns= ['Unnamed: 32', 'id'], inplace= True)\n",
    "df_raw['diagnosis'] = df_raw['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "df_raw.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start of the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base attributes: \n",
    "* 'radius'\n",
    "* 'texture'\n",
    "* 'perimeter'\n",
    "* 'area'\n",
    "* 'smoothness'\n",
    "* 'compactness'\n",
    "* 'concavity'\n",
    "* 'concave points'\n",
    "* 'symmetry'\n",
    "* 'fractal dimension'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
       "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
       "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
       "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
       "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
       "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
       "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
       "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
       "       'symmetry_worst', 'fractal_dimension_worst'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = {#concave points 0 , 0.4\n",
    "    'radius'    : [12.3, 12.5, 12.1, 12.6, np.nan, 12.2, 12.7, 12.8, 12.9, 13.0],#std-> ~3\n",
    "    'texture'   : [19.1, 18.8, 15.3, 16.8, 21.3, 20.3, 17.3, 19.2, 19.9, 18.7],  #std-> ~4\n",
    "    'perimeter' : [75.2, 76.1, 64.9, 67.0, 75.8, 84.7, 90.5, 78.0, 78.5, 79.0],  #std-> ~25\n",
    "    'area'      : [450.3, 470.3, 440.2, 380.2, np.nan, 330.1, 490.1, 500.1, 510.1, 520.1],      #std-> ~350\n",
    "    'smoothness': [0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.22],                 #std-> ~0.01\n",
    "    'compactness'   : [0.14, 0.26, 0.11, 0.25, 0.12, 0.08, 0.55, 0.16, 0.24, 0.10],             #std-> ~0.05\n",
    "    'concavity'     : [0.16, 0.33, 0.12, np.nan, np.nan, 0.15, 0.21, 0.19, 0.21, 0.12],         #std-> ~0.07\n",
    "    'concave points': [0.24, 0.23, 0.25, 0.26, np.nan, np.nan, np.nan, 0.26, 0.23, 0.24],       #std-> ~0.03\n",
    "    'symmetry'      : [0.16, 0.26, 0.15, 0.22, 0.18, np.nan, 0.23, 0.12, 0.29, 0.18],           #std-> ~0.02\n",
    "    'fractal dimension': [0.059, 0.054, 0.058, 0.059, np.nan, 0.056, 0.054, 0.058, 0.058, 0.055]#std-> ~0.007\n",
    "}\n",
    "# add nulls bcs i hate myself :D\n",
    "df_to_save = pd.DataFrame(user_input)\n",
    "#loader.save_dataframe(df= df_to_save, file_name='new_data_for_prediction', dir= 'clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../models/stacking_model_12.pkl'\n",
    "        \n",
    "base_attributes = [\n",
    "            'radius', 'texture', 'perimeter', 'area','smoothness',\n",
    "            'compactness', 'concavity', 'concave points',\n",
    "            'symmetry', 'fractal_dimension'\n",
    "        ]\n",
    "        \n",
    "ranges = { \n",
    "            # range calculation: min/~2, max * ~2\n",
    "            'radius'   : (1.0, 60),\n",
    "            'texture'  : (1, 51),\n",
    "            'perimeter': (9, 400),\n",
    "            'area'     : (50, 5001),\n",
    "            'smoothness' : (0.025, 0.32),\n",
    "            'compactness': (0.00095, 0.68),\n",
    "            'concavity'  : (0.0000, 0.8),\n",
    "            'concave points': (0.0000, 0.4),\n",
    "            'symmetry'      : (0.05, 0.6),\n",
    "            'fractal_dimension': (0.02, 0.18) \n",
    "        }\n",
    "        # sorted columns (order used in training)\n",
    "original_columns = [\n",
    "            'radius_mean', 'texture_mean', 'perimeter_mean',      # mean values\n",
    "            'area_mean', 'smoothness_mean', 'compactness_mean',\n",
    "            'concavity_mean', 'concave points_mean', 'symmetry_mean',\n",
    "            'fractal_dimension_mean',     \n",
    "            'radius_se', 'texture_se', 'perimeter_se', 'area_se', # standard error values\n",
    "            'smoothness_se',\n",
    "            'compactness_se', 'concavity_se', 'concave points_se',    \n",
    "            'symmetry_se', 'fractal_dimension_se', 'radius_worst',# worst values\n",
    "            'texture_worst', 'perimeter_worst', 'area_worst',         \n",
    "            'smoothness_worst', 'compactness_worst', 'concavity_worst',\n",
    "            'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']                         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_input(file, header= False)-> pd.DataFrame:\n",
    "        \"\"\"Process CSV file uploaded by the user to generate new data for prediction\n",
    "           - IMPORTANT: The file VALUES (measures) must have -> \n",
    "             (1) The same order as self.base_attributes\n",
    "             (2) If the column names (headers) are not added, indicate it with the check mark ✅\n",
    "             (3) Is NOT necessary to add the same amount of values for each attribute.\n",
    "                 NULL -> (without measure in any instance)\n",
    "                -- (BUT must have at least 5 values and each of them must be separated by commas)\n",
    "                -- example: if -> 'val,NULL ,val' then -> 'val',, 'val'\n",
    "            Args:\n",
    "            - uploaded_file (file): not added this param yet\n",
    "            - header (bool): if the file contains headers (important for the reading process)\"\"\"\n",
    "        try:\n",
    "            # detectar si es un csv o un xls (excel)\n",
    "            if header:\n",
    "                df = pd.read_csv(file, skipinitialspace= True)\n",
    "            else:\n",
    "                df = pd.read_csv(file, header= None)\n",
    "            \n",
    "            # validation: number of columns\n",
    "            if len(df.columns) != len(base_attributes):\n",
    "                raise ValueError(f'⚠️ ERROR: CSV file does not contain the required columns\\n',\n",
    "                                 f'- MESSAGE: in adition, make sure they are sorted as needed')\n",
    "            \n",
    "            # sort columns with order needed\n",
    "            df.columns = base_attributes\n",
    "            \n",
    "            # null treatment\n",
    "            df = df.fillna('nan')\n",
    "            dict_nulls = df.to_dict('list')\n",
    "            dict_new_data = {key: [val for val in values if val != 'nan']\n",
    "                             for key, values in dict_nulls.items()}\n",
    "            \n",
    "            return dict_new_data\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        #     st.error(f'ERROR: can not process the CSV file: {e})\n",
    "        \n",
    "#- func 01-#-#-#-#-#-#–#-#-#-#–#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#   \n",
    "def calculate_stats(measurements: List[int|float])-> Dict:\n",
    "        \"\"\"Calculate mean, Standard Error & worst(mean of the three largest values)\n",
    "        Args:\n",
    "            measurements (List): list of measurements converted to numpy array\n",
    "\n",
    "        Returns:\n",
    "            Dict: dictionary with mean, se and worst values\n",
    "        >>> measurements = [1, 2, 3, 4, 5]\n",
    "        >>> calculate_mean_se_worst(measurements)\"\"\"\n",
    "        \n",
    "        measurements_arr = np.array(measurements)\n",
    "        mean  = np.mean(measurements_arr)\n",
    "        worst = np.mean(np.sort(measurements_arr)[-3:])\n",
    "        # sort in ascending order\n",
    "        # [-3:] -> only the three largest values\n",
    "        # else -> if there are less than 3 values, return the maximum value\n",
    "        # sacar esta parte del código\n",
    "        se    = np.std(measurements_arr, ddof= 1) / np.sqrt(len(measurements_arr))\n",
    "        # ddof -> \"delta degrees of freedom\" -> '1' SAMPLE std, '0' -> POPULATION std\n",
    "        # problema: columnas \"fractal_dimension_mean, fractal_dimension_se, fractal_dimension_worst\" = NaN\n",
    "        results = {'mean': mean, 'se': se, 'worst': worst}\n",
    "        print('SUCCESS: process finished in _calculate_mean_se_worst()')\n",
    "        return results\n",
    "    \n",
    "#- func 02-#-#-#-#-#-#–#-#-#-#–#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#\n",
    "def process_values(user_data: Dict)-> pd.DataFrame:\n",
    "        \"\"\" \"\"\"\n",
    "        print(user_data)\n",
    "        features = {}\n",
    "        for attribute, measurements in user_data.items():\n",
    "            # validations\n",
    "            if not all(isinstance(val, (int, float)) for val in measurements):\n",
    "                raise ValueError(f'⚠️ ERROR: Values in {attribute} must be numerical')\n",
    "            \n",
    "            if len(measurements) < 5:\n",
    "                raise ValueError('⚠️ ERROR: At least 5 measurements are required in each attribute')\n",
    "            \n",
    "            # checkout ranges -> (if not found return -> None)\n",
    "            min_range, max_range = ranges.get(attribute, (None, None))\n",
    "            if min_range is not None and max_range is not None:\n",
    "                if not all(min_range <= val <= max_range for val in measurements):\n",
    "                    raise ValueError(f'⚠️ ERROR: Values in \"{attribute.upper()}\" are out of range'\n",
    "                                     f'\\n- Value expected between {min_range}|{max_range})')\n",
    "            \n",
    "            # calculate metrics\n",
    "            stats = calculate_stats(measurements)\n",
    "            \n",
    "            # column names\n",
    "            features[f'{attribute}_mean'] = stats['mean']\n",
    "            features[f'{attribute}_se']   = stats['se']\n",
    "            features[f'{attribute}_worst']= stats['worst']\n",
    "            \n",
    "        # df (original column order)\n",
    "        df_new_data = pd.DataFrame([features], columns= original_columns)\n",
    "        \n",
    "        return df_new_data\n",
    "    \n",
    "def prediction(file= None, frontend_input = None, header= False):\n",
    "        if file is not None:\n",
    "            dict_new_data = process_file_input(file, header)\n",
    "            print(dict_new_data)\n",
    "        else:\n",
    "             dict_new_data = frontend_input \n",
    "        \n",
    "        df_processed = process_values(user_data= dict_new_data)\n",
    "        \n",
    "        # pasamos el return a este lugar para probar el código\n",
    "        # prediction & probabilities\n",
    "        model_path = '../models/stacking_model_12.pkl'\n",
    "        model = joblib.load(model_path)\n",
    "        model.set_params(smote= 'passthrough')\n",
    "        \n",
    "        pred = model.predict(df_processed)\n",
    "        probabilities = model.predict_proba(df_processed) # only \"malignant|1\" probability\n",
    "        print(probabilities)#prob_malignant = probabilities[]\n",
    "        print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if the code works with a CSV with and without header\n",
    "csv_with_header    = '../data/clean/new_data_M_with_header.csv'\n",
    "csv_without_header = '../data/clean/new_data_B_no_header.csv'\n",
    "\n",
    "# df_processed = prediction(file= csv_with_header, header= True)\n",
    "# df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction with the entered values is: BENIGN\n",
      "Probabilities:\n",
      "- benign   : 99.351897%\n",
      "- malignant: 0.648%\n",
      "The prediction with the entered values is: MALIGNANT\n",
      "Probabilities:\n",
      "- benign   : 1.252538%\n",
      "- malignant: 98.747%\n"
     ]
    }
   ],
   "source": [
    "# class testing\n",
    "df_probe_00 = deploy.prediction(uploaded_file= csv_without_header, header= False)\n",
    "df_probe_01 = deploy.prediction(uploaded_file= csv_with_header, header= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UserWarning: [01:39:00] WARNING: /Users/runner/work/xgboost/xgboost/src/gbm/../common/error_msg.h:80: \n",
    "\n",
    "* If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
    "configuration generated by an older version of XGBoost, please export the model by calling\n",
    "`Booster.save_model` from that version first, then load it back in current version. See:\n",
    "\n",
    "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
    "\n",
    "* for more details about differences between saving model and serializing.\n",
    "\n",
    "  warnings.warn(smsg, UserWarning)\n",
    "\n",
    "### problem solved \n",
    "xgboost version:   2.1.3 *(model trained with this version)* vs 2.1.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.455556</td>\n",
       "      <td>10.57</td>\n",
       "      <td>42.47</td>\n",
       "      <td>237.944444</td>\n",
       "      <td>0.049408</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.18625</td>\n",
       "      <td>0.244286</td>\n",
       "      <td>0.198889</td>\n",
       "      <td>0.056778</td>\n",
       "      <td>...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>11.933333</td>\n",
       "      <td>49.266667</td>\n",
       "      <td>280.2</td>\n",
       "      <td>0.10163</td>\n",
       "      <td>0.353333</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.256667</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.058667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius_mean  texture_mean  perimeter_mean   area_mean  smoothness_mean  \\\n",
       "0     6.455556         10.57           42.47  237.944444         0.049408   \n",
       "\n",
       "   compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0             0.201         0.18625             0.244286       0.198889   \n",
       "\n",
       "   fractal_dimension_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0                0.056778  ...           6.8      11.933333        49.266667   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0       280.2           0.10163           0.353333             0.25   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0              0.256667            0.26                 0.058667  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input= {'radius': [6.3, 6.5, 6.1, 6.6, 6.2, 6.7, 6.8, 6.9, 6.0],\n",
    "        'texture': [9.1, 12.8, 11.3, 10.8, 9.3, 9.3, 10.3, 11.2, 9.9, 11.7],\n",
    "        'perimeter': [45.2, 36.1, 34.9, 47.0, 50.8, 44.7, 30.5, 48.0, 38.5, 49.0],\n",
    "        'area': [200.3, 270.3, 240.2, 280.2, 230.1, 290.1, 200.1, 210.1, 220.1],\n",
    "        'smoothness': [0.027011, 0.027012, 0.02702, 0.02707, 0.25073, 0.02701, 0.02706, 0.02703, 0.02705, 0.02709],\n",
    "        'compactness': [0.14, 0.26, 0.11, 0.25, 0.12, 0.08, 0.55, 0.16, 0.24, 0.1],\n",
    "        'concavity': [0.16, 0.33, 0.12, 0.15, 0.21, 0.19, 0.21, 0.12],\n",
    "        'concave points': [0.24, 0.23, 0.25, 0.26, 0.26, 0.23, 0.24],\n",
    "        'symmetry': [0.16, 0.26, 0.15, 0.22, 0.18, 0.23, 0.12, 0.29, 0.18],\n",
    "        'fractal_dimension': [0.059, 0.054, 0.058, 0.059, 0.056, 0.054, 0.058, 0.058, 0.055]}\n",
    "\n",
    "def process_values(user_data: Dict)-> pd.DataFrame:\n",
    "        \"\"\" \"\"\"\n",
    "        features = {}\n",
    "        for attribute, measurements in user_data.items():\n",
    "            # validations\n",
    "            if not all(isinstance(val, (int, float)) for val in measurements):\n",
    "                raise ValueError(f'⚠️ ERROR: Values in {attribute} must be numerical')\n",
    "                \n",
    "            if len(measurements) < 5:\n",
    "                raise ValueError('⚠️ ERROR: At least 5 measurements are required in each attribute')\n",
    "                \n",
    "            # checkout ranges -> (if not found return -> None)\n",
    "            min_range, max_range = deploy.RANGES.get(attribute, (None, None))\n",
    "            if min_range is not None and max_range is not None:\n",
    "                if not all(min_range <= val <= max_range for val in measurements):\n",
    "                    raise ValueError(f'⚠️ ERROR: Values in \"{attribute.upper()}\" are out of range'\n",
    "                                     f'\\n- Value expected between {min_range}|{max_range})')\n",
    "                \n",
    "            # calculate metrics\n",
    "            stats = deploy._calculate_stats(measurements)\n",
    "                \n",
    "            # column names\n",
    "            features[f'{attribute}_mean'] = stats['mean']\n",
    "            features[f'{attribute}_se']   = stats['se']\n",
    "            features[f'{attribute}_worst']= stats['worst']\n",
    "                \n",
    "        # df (original column order)\n",
    "        df_new_data = pd.DataFrame([features], columns= deploy.ORIGINAL_COLUMNS) \n",
    "        return df_new_data\n",
    "    \n",
    "results = process_values(input)\n",
    "results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_input( uploaded_file, header= False)-> pd.DataFrame:\n",
    "    \"\"\"Process CSV file uploaded by the user to generate new data for prediction\"\"\"\n",
    "    try:\n",
    "        file_name = uploaded_file.name.lower()\n",
    "        \n",
    "                # uploaded_file is a -> UploadedFile object not a string 'file.csv'\n",
    "        \n",
    "        if file_name.endswith('.csv'):\n",
    "            df = pd.read_csv(\n",
    "                uploaded_file, skipinitialspace= True, header= header) # estudiar cómo funciona esto y porqué 0          \n",
    "                \n",
    "        elif file_name.endswith(('.xls', '.xlsx')):\n",
    "            df = pd.read_excel(uploaded_file, engine= 'openpyxl', header= header)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('⚠️ERROR: file not suported,'\n",
    "                                    'please upload a CSV or Excel file')\n",
    "        # validation:  number of columns \n",
    "        #if not header:   \n",
    "        if len(df.columns) != len(list_dicts.BASE_ATTRIBUTES):\n",
    "            raise ValueError('ERROR: Missing columns in the file')      \n",
    "                \n",
    "        # sort columns with default names\n",
    "        df.columns = list_dicts.BASE_ATTRIBUTES\n",
    "                \n",
    "        missing_cols = set(list_dicts.BASE_ATTRIBUTES) - set(df.columns)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f'ERROR: Missing columns: {', '.join(missing_cols)}')\n",
    "                \n",
    "        # dict with base attributes\n",
    "        processed_data = {attr:[] for attr in list_dicts.BASE_ATTRIBUTES}\n",
    "                \n",
    "        for _, row in df.iterows():\n",
    "            for attr in list_dicts.BASE_ATTRIBUTES:\n",
    "                val = row[attr]\n",
    "                if pd.notna(val) and isinstance(val, (int, float)): #if not null add\n",
    "                    processed_data[attr].append(val)\n",
    "                            \n",
    "        for attr, values in processed_data.items():\n",
    "            if len(values) < 5:\n",
    "                raise ValueError('ERROR: {attr} has less than 5 values'\n",
    "                                ', add more measurements')\n",
    "        return dict_new_data\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        st.error(f'⚠️ ERROR: can not process the CSV file: {e}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
